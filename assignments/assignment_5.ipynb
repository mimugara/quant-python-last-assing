{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "Deadline: 11.06.2025 12:00 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Develop an investment strategy for the Swiss equity market, backtest it using the provided datasets (`market_data.parquet`, `jkp_data.parquet`, `spi_index.csv`) and analyze its performance by benchmarking it against the SPI index. Work with the existing code infrastructure (`qpmwp-course`) and extend it by implementing any additional components needed for the strategy. Write a report that presents your methodology and the results.\n",
    "\n",
    "### Coding (15 points)\n",
    "\n",
    "- Selection:\n",
    "  Implement selection item builder functions (via `SelectionItemBuilder`) to filter stocks based on specific criteria (e.g., exclude low-quality or high-volatility stocks).\n",
    "\n",
    "- Optimization Data & Constraints:\n",
    "  Implement functions to prepare optimization data (via `OptimizationItemBuilder`), including any econometric or machine learning-based predictions. These functions should also define optimization constraints (e.g., stock, sector, or factor exposure limits).\n",
    "\n",
    "- Optimization Model:\n",
    "  If you choose to create a custom optimization model, develop a class inheriting from Optimization (similar to `MeanVariance`, `LeastSquares`, or `BlackLitterman`). Your class should include methods set_objective and solve for defining the objective function and solving the optimization problem.\n",
    "\n",
    "- Machine Learning Prediction:\n",
    "  Integrate a machine learning model to estimate inputs for the optimization, such as expected returns or risk. This could include regression, classification, or learning-to-rank models. I suggest you to use the provided jkp_data as features, but you may also create your own (e.g., technical indicators computed on the return or price series).\n",
    "\n",
    "- Simulation:\n",
    "  Backtest the strategy and simulate portfolio returns. Account for fixed costs (1% per annum) and variable (transaction) costs (0.2% per rebalancing).\n",
    "\n",
    "\n",
    "### Report (15 points):\n",
    "\n",
    "Generate an HTML report with the following sections:\n",
    "\n",
    "- High-level strategy overview: Describe the investment strategy you developed.\n",
    "\n",
    "- Detailed explanation of the backtesting steps: Offer a more comprehensive breakdown of the backtesting process, including a description of the models implemented (e.g., details of the machine learning method used).\n",
    "\n",
    "- Backtesting results:\n",
    "    \n",
    "    - Charts: Include visual representations (e.g., cumulative performance charts, rolling 3-year returns, etc.).\n",
    "    - Descriptive statistics: Present key statistics such as mean, standard deviation, drawdown, turnover, and Sharpe ratio (or any other relevant metric) for the full backtest period as well as for subperiods (e.g., the last 5 years, or during bull vs. bear market phases).\n",
    "    - Compare your strategy against the SPI index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))   #<Change this path if needed>\n",
    "src_path = os.path.join(project_root, 'qpmwp-course\\\\src')    #<Change this path if needed>\n",
    "sys.path.append(project_root)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Local modules imports\n",
    "from helper_functions import load_data_spi, load_pickle\n",
    "from estimation.covariance import Covariance\n",
    "from estimation.expected_return import ExpectedReturn\n",
    "from optimization.optimization import Optimization, Objective, MeanVariance\n",
    "from optimization.optimization_data import OptimizationData\n",
    "from optimization.constraints import Constraints\n",
    "from backtesting.backtest_item_builder_classes import (\n",
    "    SelectionItemBuilder,\n",
    "    OptimizationItemBuilder,\n",
    ")\n",
    "from backtesting.backtest_item_builder_functions import (\n",
    "    bibfn_selection_min_volume,\n",
    "    bibfn_selection_gaps,\n",
    "    bibfn_return_series,\n",
    "    bibfn_budget_constraint,\n",
    "    bibfn_box_constraints,\n",
    "    bibfn_size_dependent_upper_bounds,\n",
    ")\n",
    "from backtesting.backtest_data import BacktestData\n",
    "from backtesting.backtest_service import BacktestService\n",
    "from backtesting.backtest import Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path_to_data = r'C:\\Users\\manco\\OneDrive\\Documentos\\UZH\\QPMwP\\quant-python-last-assing\\data' \n",
    "# <change this to your path to data>\n",
    "\n",
    "# Load market and jkp data from parquet files\n",
    "market_data = pd.read_parquet(path = f'{path_to_data}\\market_data.parquet')\n",
    "\n",
    "# Instantiate the BacktestData class\n",
    "# and set the market data and jkp data as attributes\n",
    "data = BacktestData()\n",
    "data.market_data = market_data\n",
    "data.bm_series = load_data_spi(path=r'C:\\Users\\manco\\OneDrive\\Documentos\\UZH\\QPMwP\\quant-python-last-assing\\data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading market data...\n",
      "Loading JKP factor data...\n",
      "JKP data loaded successfully with shape: (130578, 153)\n",
      "Loading SPI benchmark...\n",
      "\n",
      "Data loaded successfully!\n",
      "Market data shape: (1434082, 4)\n",
      "JKP data shape: (130578, 153)\n",
      "SPI benchmark shape: (9251,)\n",
      "\n",
      "==================================================\n",
      "DATA EXPLORATION AND QUALITY ASSESSMENT\n",
      "==================================================\n",
      "\n",
      "--- MARKET DATA STRUCTURE ---\n",
      "Columns: ['price', 'mktcap', 'liquidity', 'sector']\n",
      "Index levels: ['date', 'id']\n",
      "Date range: 1985-12-31 00:00:00 to 2024-04-30 00:00:00\n",
      "Number of unique stocks: 313\n",
      "\n",
      "--- MISSING VALUES IN MARKET DATA ---\n",
      "liquidity     105235\n",
      "sector       1431763\n",
      "dtype: int64\n",
      "\n",
      "--- RETURN SERIES ANALYSIS ---\n",
      "Return series shape: (9977, 313)\n",
      "Date range: 1986-01-01 00:00:00 to 2024-04-30 00:00:00\n",
      "\n",
      "Found 9977 extreme returns (>50% or <-50%):\n",
      "id           1  10  100  101  102  103  104  105  106  107  ...  90  91  92  \\\n",
      "date                                                        ...               \n",
      "1986-01-01 NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... NaN NaN NaN   \n",
      "1986-01-02 NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... NaN NaN NaN   \n",
      "1986-01-03 NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... NaN NaN NaN   \n",
      "1986-01-06 NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... NaN NaN NaN   \n",
      "1986-01-07 NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ... NaN NaN NaN   \n",
      "\n",
      "id          93  94  95  96  97  98  99  \n",
      "date                                    \n",
      "1986-01-01 NaN NaN NaN NaN NaN NaN NaN  \n",
      "1986-01-02 NaN NaN NaN NaN NaN NaN NaN  \n",
      "1986-01-03 NaN NaN NaN NaN NaN NaN NaN  \n",
      "1986-01-06 NaN NaN NaN NaN NaN NaN NaN  \n",
      "1986-01-07 NaN NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[5 rows x 313 columns]\n",
      "\n",
      "--- VOLUME/LIQUIDITY ANALYSIS ---\n",
      "Volume series shape: (9207, 313)\n",
      "\n",
      "Top 10 stocks by median daily volume:\n",
      "id\n",
      "24     3.558355e+08\n",
      "48     3.511441e+08\n",
      "33     3.207774e+08\n",
      "36     1.961317e+08\n",
      "129    1.692435e+08\n",
      "185    1.544339e+08\n",
      "149    1.190398e+08\n",
      "40     9.716716e+07\n",
      "122    8.171582e+07\n",
      "210    7.732494e+07\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 stocks by median daily volume:\n",
      "id\n",
      "82     8005.0\n",
      "152    6700.0\n",
      "1      4750.0\n",
      "15     1280.0\n",
      "274       0.0\n",
      "277       0.0\n",
      "306       0.0\n",
      "290       0.0\n",
      "14        0.0\n",
      "313       0.0\n",
      "dtype: float64\n",
      "\n",
      "Stocks with most zero-volume days:\n",
      "id\n",
      "152    2489\n",
      "196    2471\n",
      "277    2092\n",
      "14     1719\n",
      "147    1628\n",
      "32     1582\n",
      "214    1454\n",
      "313    1391\n",
      "34     1358\n",
      "306    1125\n",
      "dtype: int64\n",
      "\n",
      "--- JKP FACTOR DATA ANALYSIS ---\n",
      "JKP data columns: ['niq_su', 'ret_6_1', 'ret_12_1', 'saleq_su', 'tax_gr1a', 'ni_inc8q', 'prc_highprc_252d', 'resff3_6_1', 'resff3_12_1', 'be_me', 'debt_me', 'at_me', 'ret_60_12', 'ni_me', 'fcf_me', 'div12m_me', 'eqpo_me', 'eqnpo_me', 'sale_gr3', 'sale_gr1', 'ebitda_mev', 'sale_me', 'ocf_me', 'ival_me', 'bev_mev', 'netdebt_me', 'eq_dur', 'capex_abn', 'at_gr1', 'ppeinv_gr1a', 'noa_at', 'noa_gr1a', 'lnoa_gr1a', 'capx_gr1', 'capx_gr2', 'capx_gr3', 'chcsho_12m', 'eqnpo_12m', 'debt_gr3', 'inv_gr1', 'inv_gr1a', 'oaccruals_at', 'taccruals_at', 'cowc_gr1a', 'coa_gr1a', 'col_gr1a', 'nncoa_gr1a', 'ncoa_gr1a', 'ncol_gr1a', 'nfna_gr1a', 'sti_gr1a', 'lti_gr1a', 'fnl_gr1a', 'be_gr1a', 'oaccruals_ni', 'taccruals_ni', 'netis_at', 'eqnetis_at', 'dbnetis_at', 'niq_be', 'niq_be_chg1', 'niq_at', 'niq_at_chg1', 'ebit_bev', 'ebit_sale', 'sale_bev', 'at_turnover', 'gp_at', 'gp_atl1', 'ope_be', 'ope_bel1', 'op_at', 'op_atl1', 'cop_at', 'cop_atl1', 'f_score', 'o_score', 'z_score', 'pi_nix', 'at_be', 'saleq_gr1', 'rd_me', 'rd_sale', 'opex_at', 'emp_gr1', 'rd5_at', 'age', 'dsale_dinv', 'dsale_drec', 'dgp_dsale', 'dsale_dsga', 'sale_emp_gr1', 'tangibility', 'kz_index', 'ocfq_saleq_std', 'cash_at', 'ni_ar1', 'ni_ivol', 'earnings_variability', 'aliq_at', 'aliq_mat', 'seas_1_1an', 'seas_1_1na', 'seas_2_5an', 'seas_2_5na', 'seas_6_10an', 'seas_6_10na', 'seas_11_15an', 'seas_11_15na', 'seas_16_20an', 'seas_16_20na', 'market_equity', 'ivol_ff3_21d', 'ivol_capm_252d', 'ivol_capm_21d', 'ivol_hxz4_21d', 'rvol_21d', 'beta_60m', 'betabab_1260d', 'beta_dimson_21d', 'turnover_126d', 'turnover_var_126d', 'dolvol_126d', 'dolvol_var_126d', 'prc', 'ami_126d', 'zero_trades_21d', 'zero_trades_126d', 'zero_trades_252d', 'rmax1_21d', 'rskew_21d', 'iskew_capm_21d', 'iskew_ff3_21d', 'iskew_hxz4_21d', 'coskew_21d', 'ret_1_0', 'betadown_252d', 'bidaskhl_21d', 'ret_3_1', 'ret_9_1', 'ret_12_7', 'corr_1260d', 'rmax5_21d', 'rmax5_rvol_21d', 'ni_be', 'ocf_at', 'ocf_at_chg1', 'mispricing_perf', 'mispricing_mgmt', 'qmj', 'qmj_prof', 'qmj_growth', 'qmj_safety']\n",
      "JKP date range: 1985-12-31 00:00:00 to 2024-03-31 00:00:00\n",
      "\n",
      "Missing values in JKP factors:\n",
      "niq_su              74902\n",
      "ret_6_1              7996\n",
      "ret_12_1            13360\n",
      "saleq_su            94031\n",
      "tax_gr1a            32091\n",
      "ni_inc8q            78166\n",
      "prc_highprc_252d    29618\n",
      "resff3_6_1          36333\n",
      "resff3_12_1         35861\n",
      "be_me               34862\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "DATA QUALITY FILTERS AND PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "Creating technical features...\n",
      "\n",
      "--- CREATING TECHNICAL FEATURES (FAST VERSION) ---\n",
      "Computing momentum indicators...\n",
      "  - momentum_1m completed\n",
      "  - momentum_3m completed\n",
      "  - momentum_6m completed\n",
      "  - momentum_12m completed\n",
      "Computing volatility indicators...\n",
      "  - volatility_1m completed\n",
      "  - volatility_3m completed\n",
      "  - volatility_6m completed\n",
      "Computing volume indicators...\n",
      "  - Volume indicators completed\n",
      "Computing market timing indicators...\n",
      "  - Market timing indicators completed\n",
      "✅ All technical features completed!\n",
      "Technical features created with shape: (9977, 11)\n",
      "Feature columns: ['momentum_1m', 'momentum_3m', 'momentum_6m', 'momentum_12m', 'volatility_1m', 'volatility_3m', 'volatility_6m', 'avg_volume', 'volume_trend', 'market_breadth', 'market_dispersion']\n",
      "\n",
      "==================================================\n",
      "PREPARING SELECTION ITEM BUILDERS\n",
      "==================================================\n",
      "Selection item builders created:\n",
      "  - data_quality\n",
      "  - market_cap_tiers\n",
      "  - volume_consistency\n",
      "  - trading_gaps\n",
      "\n",
      "==================================================\n",
      "TESTING SELECTION FILTERS\n",
      "==================================================\n",
      "Testing selection filters for date: 2024-02-07\n",
      "\n",
      "Testing data quality filter...\n",
      "Data quality filter results shape: (313, 11)\n",
      "Stocks passing all filters: 49\n",
      "Filter breakdown:\n",
      "  volume_filter: 104 stocks\n",
      "  price_filter: 214 stocks\n",
      "  trading_filter: 167 stocks\n",
      "  gap_filter: 305 stocks\n",
      "  completeness_filter: 177 stocks\n",
      "\n",
      "Testing market cap filter...\n",
      "Market cap filter results shape: (313, 3)\n",
      "Stocks by tier:\n",
      "tier\n",
      "mid      113\n",
      "small     91\n",
      "micro     73\n",
      "large     36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "DATA PREPROCESSING SUMMARY\n",
      "==================================================\n",
      "\n",
      "✅ Data Loading:\n",
      "  - Market data: 1,434,082 rows\n",
      "  - Unique stocks: 313\n",
      "  - Date range: 1985-12-31 00:00:00 to 2024-04-30 00:00:00\n",
      "  - JKP factors: 153 factors available\n",
      "\n",
      "✅ Data Quality Filters Created:\n",
      "  - Comprehensive data quality filter (volume, price, completeness)\n",
      "  - Market cap tier classification\n",
      "  - Volume consistency filters\n",
      "  - Trading gap analysis\n",
      "\n",
      "✅ Technical Features:\n",
      "  - 11 technical indicators created\n",
      "  - Momentum, volatility, volume, and market timing features\n",
      "\n",
      "📋 Next Steps for Strategy Development:\n",
      "  1. Machine Learning Model Development\n",
      "     - Use technical features + JKP factors for return prediction\n",
      "     - Consider ensemble methods (XGBoost, Random Forest)\n",
      "     - Implement cross-validation with time series splits\n",
      "\n",
      "  2. Portfolio Optimization\n",
      "     - Implement custom optimization class\n",
      "     - Add sector/factor exposure constraints\n",
      "     - Include transaction cost modeling\n",
      "\n",
      "  3. Risk Management\n",
      "     - Position sizing based on volatility\n",
      "     - Maximum drawdown controls\n",
      "     - Sector concentration limits\n",
      "\n",
      "  4. Backtesting Framework\n",
      "     - Monthly or quarterly rebalancing\n",
      "     - Out-of-sample testing\n",
      "     - Performance attribution analysis\n",
      "\n",
      "🎯 Ready to proceed with strategy implementation!\n",
      "   Estimated investable universe: ~49 stocks\n",
      "   Strategy focus: Swiss equity market with systematic factor approach\n"
     ]
    }
   ],
   "source": [
    "# Assignment 5 - Data Preprocessing for Swiss Equity Strategy\n",
    "# Quantitative Portfolio Management with Python\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))   #<Change this path if needed>\n",
    "src_path = os.path.join(project_root, 'qpmwp-course\\\\src')    #<Change this path if needed>\n",
    "sys.path.append(project_root)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Local modules imports\n",
    "from helper_functions import load_data_spi, load_pickle\n",
    "from estimation.covariance import Covariance\n",
    "from estimation.expected_return import ExpectedReturn\n",
    "from optimization.optimization import Optimization, Objective, MeanVariance\n",
    "from optimization.optimization_data import OptimizationData\n",
    "from optimization.constraints import Constraints\n",
    "from backtesting.backtest_item_builder_classes import (\n",
    "    SelectionItemBuilder,\n",
    "    OptimizationItemBuilder,\n",
    ")\n",
    "from backtesting.backtest_item_builder_functions import (\n",
    "    bibfn_selection_min_volume,\n",
    "    bibfn_selection_gaps,\n",
    "    bibfn_return_series,\n",
    "    bibfn_budget_constraint,\n",
    "    bibfn_box_constraints,\n",
    "    bibfn_size_dependent_upper_bounds,\n",
    ")\n",
    "from backtesting.backtest_data import BacktestData\n",
    "from backtesting.backtest_service import BacktestService\n",
    "from backtesting.backtest import Backtest\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD AND INSPECT DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load data\n",
    "path_to_data = r'C:\\Users\\manco\\OneDrive\\Documentos\\UZH\\QPMwP\\quant-python-last-assing\\data' \n",
    "# <change this to your path to data>\n",
    "\n",
    "# Load market and jkp data from parquet files\n",
    "print(\"Loading market data...\")\n",
    "market_data = pd.read_parquet(path=f'{path_to_data}\\\\market_data.parquet')\n",
    "\n",
    "print(\"Loading JKP factor data...\")\n",
    "try:\n",
    "    jkp_data = pd.read_parquet(path=f'{path_to_data}\\\\jkp_data.parquet')\n",
    "    has_jkp_data = True\n",
    "    print(f\"JKP data loaded successfully with shape: {jkp_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"JKP data not found - proceeding without factor data\")\n",
    "    jkp_data = None\n",
    "    has_jkp_data = False\n",
    "\n",
    "# Instantiate the BacktestData class\n",
    "data = BacktestData()\n",
    "data.market_data = market_data\n",
    "if has_jkp_data:\n",
    "    data.jkp_data = jkp_data\n",
    "\n",
    "# Load SPI benchmark\n",
    "print(\"Loading SPI benchmark...\")\n",
    "data.bm_series = load_data_spi(path=path_to_data)\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Market data shape: {market_data.shape}\")\n",
    "if has_jkp_data:\n",
    "    print(f\"JKP data shape: {jkp_data.shape}\")\n",
    "print(f\"SPI benchmark shape: {data.bm_series.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA EXPLORATION AND QUALITY ASSESSMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA EXPLORATION AND QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Explore market data structure\n",
    "print(\"\\n--- MARKET DATA STRUCTURE ---\")\n",
    "print(\"Columns:\", market_data.columns.tolist())\n",
    "print(\"Index levels:\", market_data.index.names)\n",
    "print(\"Date range:\", market_data.index.get_level_values('date').min(), \n",
    "      \"to\", market_data.index.get_level_values('date').max())\n",
    "print(\"Number of unique stocks:\", market_data.index.get_level_values('id').nunique())\n",
    "\n",
    "# Check for missing values in market data\n",
    "print(\"\\n--- MISSING VALUES IN MARKET DATA ---\")\n",
    "missing_summary = market_data.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Analyze return series quality\n",
    "print(\"\\n--- RETURN SERIES ANALYSIS ---\")\n",
    "return_series = data.get_return_series()\n",
    "print(f\"Return series shape: {return_series.shape}\")\n",
    "print(f\"Date range: {return_series.index.min()} to {return_series.index.max()}\")\n",
    "\n",
    "# Check for extreme returns (potential data issues)\n",
    "extreme_returns = return_series[(return_series > 0.5) | (return_series < -0.5)]\n",
    "if not extreme_returns.empty:\n",
    "    print(f\"\\nFound {len(extreme_returns)} extreme returns (>50% or <-50%):\")\n",
    "    print(extreme_returns.head())\n",
    "\n",
    "# Volume analysis\n",
    "print(\"\\n--- VOLUME/LIQUIDITY ANALYSIS ---\")\n",
    "volume_series = data.get_volume_series()\n",
    "print(f\"Volume series shape: {volume_series.shape}\")\n",
    "\n",
    "# Calculate median daily volume by stock\n",
    "median_volumes = volume_series.median(axis=0).sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 stocks by median daily volume:\")\n",
    "print(median_volumes.head(10))\n",
    "print(f\"\\nBottom 10 stocks by median daily volume:\")\n",
    "print(median_volumes.tail(10))\n",
    "\n",
    "# Count zero volume days\n",
    "zero_volume_days = (volume_series == 0).sum(axis=0).sort_values(ascending=False)\n",
    "print(f\"\\nStocks with most zero-volume days:\")\n",
    "print(zero_volume_days.head(10))\n",
    "\n",
    "# JKP data exploration (if available)\n",
    "if has_jkp_data:\n",
    "    print(\"\\n--- JKP FACTOR DATA ANALYSIS ---\")\n",
    "    print(\"JKP data columns:\", jkp_data.columns.tolist())\n",
    "    print(\"JKP date range:\", jkp_data.index.get_level_values('date').min(), \n",
    "          \"to\", jkp_data.index.get_level_values('date').max())\n",
    "    \n",
    "    # Check missing values in JKP data\n",
    "    jkp_missing = jkp_data.isnull().sum()\n",
    "    print(\"\\nMissing values in JKP factors:\")\n",
    "    print(jkp_missing[jkp_missing > 0].head(10))\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATA QUALITY FILTERS AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA QUALITY FILTERS AND PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive data quality filters\n",
    "def create_data_quality_filters():\n",
    "    \"\"\"\n",
    "    Create a comprehensive set of data quality filters for stock selection\n",
    "    \"\"\"\n",
    "    \n",
    "    def bibfn_selection_data_quality(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Comprehensive data quality filter combining multiple criteria\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "        min_volume = kwargs.get('min_volume', 500_000)\n",
    "        min_price = kwargs.get('min_price', 1.0)  # CHF\n",
    "        max_price = kwargs.get('max_price', 1000.0)  # CHF\n",
    "        min_trading_days = kwargs.get('min_trading_days', 200)  # out of ~252 trading days\n",
    "        max_gap_days = kwargs.get('max_gap_days', 10)  # consecutive zero volume days\n",
    "        lookback_days = kwargs.get('lookback_days', 252)\n",
    "        \n",
    "        # Get return and volume data\n",
    "        return_data = bs.data.get_return_series(\n",
    "            end_date=rebdate,\n",
    "            width=lookback_days,\n",
    "            fillna_value=None\n",
    "        )\n",
    "        \n",
    "        volume_data = bs.data.get_volume_series(\n",
    "            end_date=rebdate,\n",
    "            width=lookback_days\n",
    "        )\n",
    "        \n",
    "        # Get price data (reconstruct from returns)\n",
    "        price_data = (1 + return_data.fillna(0)).cumprod()\n",
    "        latest_prices = price_data.iloc[-1]\n",
    "        \n",
    "        # Initialize results\n",
    "        stocks = return_data.columns\n",
    "        results = pd.DataFrame(index=stocks)\n",
    "        \n",
    "        # Filter 1: Minimum volume\n",
    "        median_volume = volume_data.median(axis=0)\n",
    "        volume_filter = median_volume >= min_volume\n",
    "        results['volume_filter'] = volume_filter.astype(int)\n",
    "        \n",
    "        # Filter 2: Price range\n",
    "        price_filter = (latest_prices >= min_price) & (latest_prices <= max_price)\n",
    "        results['price_filter'] = price_filter.astype(int)\n",
    "        \n",
    "        # Filter 3: Minimum trading days (non-zero volume)\n",
    "        trading_days = (volume_data > 0).sum(axis=0)\n",
    "        trading_filter = trading_days >= min_trading_days\n",
    "        results['trading_filter'] = trading_filter.astype(int)\n",
    "        \n",
    "        # Filter 4: Maximum gap days (consecutive zero volumes)\n",
    "        def max_consecutive_zeros(series):\n",
    "            \"\"\"Calculate maximum consecutive zeros in a series\"\"\"\n",
    "            if series.empty:\n",
    "                return 0\n",
    "            zero_groups = (series == 0).astype(int).groupby(series.ne(0).astype(int).cumsum()).sum()\n",
    "            return zero_groups.max() if not zero_groups.empty else 0\n",
    "        \n",
    "        max_gaps = volume_data.apply(max_consecutive_zeros, axis=0)\n",
    "        gap_filter = max_gaps <= max_gap_days\n",
    "        results['gap_filter'] = gap_filter.astype(int)\n",
    "        \n",
    "        # Filter 5: Data completeness (limit missing returns)\n",
    "        missing_returns = return_data.isnull().sum(axis=0)\n",
    "        max_missing = int(0.1 * lookback_days)  # Allow max 10% missing data\n",
    "        completeness_filter = missing_returns <= max_missing\n",
    "        results['completeness_filter'] = completeness_filter.astype(int)\n",
    "        \n",
    "        # Combine all filters\n",
    "        all_filters = [\n",
    "            'volume_filter', 'price_filter', 'trading_filter', \n",
    "            'gap_filter', 'completeness_filter'\n",
    "        ]\n",
    "        results['binary'] = results[all_filters].min(axis=1)\n",
    "        \n",
    "        # Store intermediate values for analysis\n",
    "        results['median_volume'] = median_volume\n",
    "        results['latest_price'] = latest_prices\n",
    "        results['trading_days'] = trading_days\n",
    "        results['max_gap_days'] = max_gaps\n",
    "        results['missing_returns'] = missing_returns\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return bibfn_selection_data_quality\n",
    "\n",
    "# Create enhanced selection functions\n",
    "def create_enhanced_selection_functions():\n",
    "    \"\"\"\n",
    "    Create enhanced selection functions for different market conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    def bibfn_selection_market_cap_tiers(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Classify stocks into market cap tiers and apply tier-specific filters\n",
    "        \"\"\"\n",
    "        # Get market cap data\n",
    "        mcap_data = bs.data.market_data['mktcap']\n",
    "        mcap_recent = mcap_data[mcap_data.index.get_level_values('date') <= rebdate].groupby('id').last()\n",
    "        \n",
    "        # Define market cap tiers (in CHF)\n",
    "        large_cap_threshold = 10_000_000_000   # 10B CHF\n",
    "        mid_cap_threshold = 1_000_000_000      # 1B CHF\n",
    "        small_cap_threshold = 300_000_000      # 300M CHF\n",
    "        \n",
    "        results = pd.DataFrame(index=mcap_recent.index)\n",
    "        results['market_cap'] = mcap_recent\n",
    "        \n",
    "        # Classify tiers\n",
    "        results['tier'] = 'micro'\n",
    "        results.loc[mcap_recent >= small_cap_threshold, 'tier'] = 'small'\n",
    "        results.loc[mcap_recent >= mid_cap_threshold, 'tier'] = 'mid'\n",
    "        results.loc[mcap_recent >= large_cap_threshold, 'tier'] = 'large'\n",
    "        \n",
    "        # Apply tier-specific selection (focus on investable universe)\n",
    "        tier_filter = mcap_recent >= small_cap_threshold  # Exclude micro caps\n",
    "        results['binary'] = tier_filter.astype(int)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def bibfn_selection_sector_diversification(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ensure sector diversification by limiting concentration\n",
    "        \"\"\"\n",
    "        max_sector_weight = kwargs.get('max_sector_weight', 0.4)\n",
    "        \n",
    "        # This is a placeholder - in practice you'd need sector classification data\n",
    "        # For now, we'll create a dummy implementation\n",
    "        selected_stocks = bs.selection.selected if hasattr(bs, 'selection') else []\n",
    "        \n",
    "        results = pd.DataFrame(index=selected_stocks)\n",
    "        results['binary'] = 1  # Accept all for now\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return bibfn_selection_market_cap_tiers, bibfn_selection_sector_diversification\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TECHNICAL INDICATORS AND FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "def create_technical_features(return_series, volume_series, lookback_window=252):\n",
    "    \"\"\"\n",
    "    Create technical analysis features for stock selection and prediction - OPTIMIZED VERSION\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CREATING TECHNICAL FEATURES (FAST VERSION) ---\")\n",
    "    \n",
    "    # Initialize features dataframe\n",
    "    features = pd.DataFrame(index=return_series.index)\n",
    "    \n",
    "    # 1. Momentum indicators - MUCH FASTER METHOD\n",
    "    print(\"Computing momentum indicators...\")\n",
    "    \n",
    "    # Use cumulative product method - much faster than apply()\n",
    "    cum_returns = (1 + return_series.fillna(0)).cumprod()\n",
    "    \n",
    "    for months in [1, 3, 6, 12]:\n",
    "        days = months * 21\n",
    "        momentum_col = f'momentum_{months}m'\n",
    "        # Calculate momentum as (current_price / past_price) - 1\n",
    "        momentum = cum_returns / cum_returns.shift(days) - 1\n",
    "        features[momentum_col] = momentum.mean(axis=1, skipna=True)\n",
    "        print(f\"  - {momentum_col} completed\")\n",
    "    \n",
    "    # 2. Volatility indicators - OPTIMIZED\n",
    "    print(\"Computing volatility indicators...\")\n",
    "    \n",
    "    for months in [1, 3, 6]:\n",
    "        days = months * 21\n",
    "        vol_col = f'volatility_{months}m'\n",
    "        # Much faster rolling std calculation\n",
    "        volatility = return_series.rolling(window=days, min_periods=int(days*0.7)).std() * np.sqrt(252)\n",
    "        features[vol_col] = volatility.mean(axis=1, skipna=True)\n",
    "        print(f\"  - {vol_col} completed\")\n",
    "    \n",
    "    # 3. Volume-based indicators - SIMPLIFIED\n",
    "    print(\"Computing volume indicators...\")\n",
    "    \n",
    "    # Simple moving averages - much faster\n",
    "    volume_ma_short = volume_series.rolling(window=21, min_periods=15).mean()\n",
    "    volume_ma_long = volume_series.rolling(window=63, min_periods=45).mean()\n",
    "    \n",
    "    features['avg_volume'] = volume_ma_short.mean(axis=1, skipna=True)\n",
    "    # Volume trend as ratio of short to long MA (simpler than polyfit)\n",
    "    features['volume_trend'] = (volume_ma_short / volume_ma_long - 1).mean(axis=1, skipna=True)\n",
    "    print(\"  - Volume indicators completed\")\n",
    "    \n",
    "    # 4. Market timing indicators - ALREADY FAST\n",
    "    print(\"Computing market timing indicators...\")\n",
    "    \n",
    "    # Market breadth (percentage of stocks with positive returns)\n",
    "    features['market_breadth'] = (return_series > 0).mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Market dispersion (cross-sectional volatility)\n",
    "    features['market_dispersion'] = return_series.std(axis=1, skipna=True)\n",
    "    print(\"  - Market timing indicators completed\")\n",
    "    \n",
    "    print(\"✅ All technical features completed!\")\n",
    "    return features\n",
    "\n",
    "# Create technical features\n",
    "print(\"\\nCreating technical features...\")\n",
    "technical_features = create_technical_features(\n",
    "    data.get_return_series(),\n",
    "    data.get_volume_series()\n",
    ")\n",
    "\n",
    "print(f\"Technical features created with shape: {technical_features.shape}\")\n",
    "print(\"Feature columns:\", technical_features.columns.tolist())\n",
    "\n",
    "# =============================================================================\n",
    "# 5. PREPARE SELECTION ITEM BUILDERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING SELECTION ITEM BUILDERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get selection functions\n",
    "data_quality_filter = create_data_quality_filters()\n",
    "market_cap_filter, sector_filter = create_enhanced_selection_functions()\n",
    "\n",
    "# Define comprehensive selection item builders\n",
    "selection_item_builders = {\n",
    "    'data_quality': SelectionItemBuilder(\n",
    "        bibfn=data_quality_filter,\n",
    "        min_volume=1_000_000,        # 1M CHF daily volume\n",
    "        min_price=5.0,               # Minimum 5 CHF per share\n",
    "        max_price=500.0,             # Maximum 500 CHF per share\n",
    "        min_trading_days=200,        # At least 200 trading days per year\n",
    "        max_gap_days=5,              # Max 5 consecutive non-trading days\n",
    "        lookback_days=252            # 1 year lookback\n",
    "    ),\n",
    "    \n",
    "    'market_cap_tiers': SelectionItemBuilder(\n",
    "        bibfn=market_cap_filter,\n",
    "    ),\n",
    "    \n",
    "    'volume_consistency': SelectionItemBuilder(\n",
    "        bibfn=bibfn_selection_min_volume,\n",
    "        width=126,                   # 6 months\n",
    "        min_volume=500_000,          # 500k CHF\n",
    "        agg_fn=np.median,\n",
    "    ),\n",
    "    \n",
    "    'trading_gaps': SelectionItemBuilder(\n",
    "        bibfn=bibfn_selection_gaps,\n",
    "        width=252,                   # 1 year\n",
    "        n_days=7,                    # Max 7 consecutive zero-volume days\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Selection item builders created:\")\n",
    "for name, builder in selection_item_builders.items():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TEST SELECTION FILTERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING SELECTION FILTERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up test date\n",
    "test_dates = data.get_return_series().index[-252:]  # Last year of data\n",
    "test_date = test_dates[-60].strftime('%Y-%m-%d')    # 60 days from end\n",
    "\n",
    "print(f\"Testing selection filters for date: {test_date}\")\n",
    "\n",
    "# Create a simple backtest service for testing\n",
    "class SimpleBacktestService:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.selection = type('Selection', (), {\n",
    "            'selected': [],\n",
    "            'filtered': {},\n",
    "            'add_filtered': lambda self, name, value: setattr(self, 'filtered', {**getattr(self, 'filtered', {}), name: value})\n",
    "        })()\n",
    "\n",
    "# Test the selection filters\n",
    "bs_test = SimpleBacktestService(data)\n",
    "\n",
    "print(\"\\nTesting data quality filter...\")\n",
    "try:\n",
    "    quality_results = data_quality_filter(bs_test, test_date)\n",
    "    print(f\"Data quality filter results shape: {quality_results.shape}\")\n",
    "    print(f\"Stocks passing all filters: {quality_results['binary'].sum()}\")\n",
    "    print(f\"Filter breakdown:\")\n",
    "    filter_cols = [col for col in quality_results.columns if col.endswith('_filter')]\n",
    "    for col in filter_cols:\n",
    "        print(f\"  {col}: {quality_results[col].sum()} stocks\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing data quality filter: {e}\")\n",
    "\n",
    "print(\"\\nTesting market cap filter...\")\n",
    "try:\n",
    "    mcap_results = market_cap_filter(bs_test, test_date)\n",
    "    print(f\"Market cap filter results shape: {mcap_results.shape}\")\n",
    "    print(f\"Stocks by tier:\")\n",
    "    print(mcap_results['tier'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Error testing market cap filter: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. SUMMARY AND NEXT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n✅ Data Loading:\")\n",
    "print(f\"  - Market data: {market_data.shape[0]:,} rows\")\n",
    "print(f\"  - Unique stocks: {market_data.index.get_level_values('id').nunique():,}\")\n",
    "print(f\"  - Date range: {market_data.index.get_level_values('date').min()} to {market_data.index.get_level_values('date').max()}\")\n",
    "if has_jkp_data:\n",
    "    print(f\"  - JKP factors: {jkp_data.shape[1]} factors available\")\n",
    "\n",
    "print(\"\\n✅ Data Quality Filters Created:\")\n",
    "print(\"  - Comprehensive data quality filter (volume, price, completeness)\")\n",
    "print(\"  - Market cap tier classification\")\n",
    "print(\"  - Volume consistency filters\")\n",
    "print(\"  - Trading gap analysis\")\n",
    "\n",
    "print(\"\\n✅ Technical Features:\")\n",
    "print(f\"  - {len(technical_features.columns)} technical indicators created\")\n",
    "print(\"  - Momentum, volatility, volume, and market timing features\")\n",
    "\n",
    "print(\"\\n📋 Next Steps for Strategy Development:\")\n",
    "print(\"  1. Machine Learning Model Development\")\n",
    "print(\"     - Use technical features + JKP factors for return prediction\")\n",
    "print(\"     - Consider ensemble methods (XGBoost, Random Forest)\")\n",
    "print(\"     - Implement cross-validation with time series splits\")\n",
    "print(\"\\n  2. Portfolio Optimization\")\n",
    "print(\"     - Implement custom optimization class\")\n",
    "print(\"     - Add sector/factor exposure constraints\")\n",
    "print(\"     - Include transaction cost modeling\")\n",
    "print(\"\\n  3. Risk Management\")\n",
    "print(\"     - Position sizing based on volatility\")\n",
    "print(\"     - Maximum drawdown controls\")\n",
    "print(\"     - Sector concentration limits\")\n",
    "print(\"\\n  4. Backtesting Framework\")\n",
    "print(\"     - Monthly or quarterly rebalancing\")\n",
    "print(\"     - Out-of-sample testing\")\n",
    "print(\"     - Performance attribution analysis\")\n",
    "\n",
    "print(f\"\\n🎯 Ready to proceed with strategy implementation!\")\n",
    "print(f\"   Estimated investable universe: ~{quality_results['binary'].sum() if 'quality_results' in locals() else 'TBD'} stocks\")\n",
    "print(f\"   Strategy focus: Swiss equity market with systematic factor approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing Enhanced Selection Pipeline - FIXED VERSION\n",
      "==================================================\n",
      "❌ Test failed: name 'data' is not defined\n",
      "Please check that the selection functions are properly loaded\n"
     ]
    }
   ],
   "source": [
    "# Assignment 5 - Enhanced Selection Item Builder Functions\n",
    "# Implementing sophisticated stock selection criteria\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. QUALITY-BASED SELECTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def bibfn_selection_comprehensive_quality(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive quality filter combining multiple criteria with sophisticated logic\n",
    "    \"\"\"\n",
    "    # Parameters with more reasonable defaults\n",
    "    min_volume = kwargs.get('min_volume', 500_000)    # 500K CHF daily (was 1M)\n",
    "    min_price = kwargs.get('min_price', 1.0)          # 1 CHF minimum (was 5.0)\n",
    "    max_price = kwargs.get('max_price', 2000.0)       # 2000 CHF maximum (was 500)\n",
    "    min_market_cap = kwargs.get('min_market_cap', 100_000_000)  # 100M CHF (was 300M)\n",
    "    min_trading_days = kwargs.get('min_trading_days', 150)      # 150/252 days (was 200)\n",
    "    max_gap_days = kwargs.get('max_gap_days', 20)               # Max 20 consecutive gaps (was 10)\n",
    "    lookback_days = kwargs.get('lookback_days', 252)            # 1 year lookback\n",
    "    volatility_threshold = kwargs.get('volatility_threshold', 1.0)  # 100% annualized (was 60%)\n",
    "    \n",
    "    # Get data\n",
    "    return_data = bs.data.get_return_series(\n",
    "        end_date=rebdate, width=lookback_days, fillna_value=None\n",
    "    )\n",
    "    volume_data = bs.data.get_volume_series(\n",
    "        end_date=rebdate, width=lookback_days\n",
    "    )\n",
    "    \n",
    "    # Get market cap data\n",
    "    mcap_data = bs.data.market_data['mktcap']\n",
    "    mcap_recent = mcap_data[\n",
    "        mcap_data.index.get_level_values('date') <= rebdate\n",
    "    ].groupby('id').last()\n",
    "    \n",
    "    # Reconstruct price data\n",
    "    price_data = (1 + return_data.fillna(0)).cumprod()\n",
    "    latest_prices = price_data.iloc[-1]\n",
    "    \n",
    "    # Initialize results\n",
    "    stocks = return_data.columns\n",
    "    results = pd.DataFrame(index=stocks)\n",
    "    \n",
    "    print(f\"Evaluating {len(stocks)} stocks for date {rebdate}\")\n",
    "    \n",
    "    # === FILTER 1: MARKET CAP ===\n",
    "    mcap_filter = mcap_recent.reindex(stocks, fill_value=0) >= min_market_cap\n",
    "    results['market_cap'] = mcap_recent.reindex(stocks, fill_value=0)\n",
    "    results['mcap_filter'] = mcap_filter.astype(int)\n",
    "    print(f\"  Market cap filter (≥{min_market_cap/1e6:.0f}M): {mcap_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 2: PRICE RANGE ===\n",
    "    # More lenient price filter - check for reasonable price levels\n",
    "    price_filter = (latest_prices >= min_price) & (latest_prices <= max_price) & (latest_prices > 0)\n",
    "    results['latest_price'] = latest_prices\n",
    "    results['price_filter'] = price_filter.astype(int)\n",
    "    print(f\"  Price filter ({min_price}-{max_price} CHF): {price_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 3: LIQUIDITY ===\n",
    "    median_volume = volume_data.median(axis=0, skipna=True)\n",
    "    volume_filter = median_volume >= min_volume\n",
    "    results['median_volume'] = median_volume\n",
    "    results['volume_filter'] = volume_filter.astype(int)\n",
    "    print(f\"  Volume filter (≥{min_volume/1e6:.1f}M): {volume_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 4: TRADING CONSISTENCY ===\n",
    "    # Count non-zero volume days\n",
    "    trading_days = (volume_data > 0).sum(axis=0)\n",
    "    trading_filter = trading_days >= min_trading_days\n",
    "    results['trading_days'] = trading_days\n",
    "    results['trading_filter'] = trading_filter.astype(int)\n",
    "    print(f\"  Trading consistency (≥{min_trading_days} days): {trading_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 5: MAXIMUM GAPS ===\n",
    "    def max_consecutive_zeros(series):\n",
    "        if series.empty or series.isna().all():\n",
    "            return float('inf')\n",
    "        zero_mask = (series == 0) | series.isna()\n",
    "        if not zero_mask.any():\n",
    "            return 0\n",
    "        # Group consecutive zeros and find max length\n",
    "        groups = zero_mask.astype(int).groupby((~zero_mask).cumsum()).sum()\n",
    "        return groups.max() if not groups.empty else 0\n",
    "    \n",
    "    max_gaps = volume_data.apply(max_consecutive_zeros, axis=0)\n",
    "    gap_filter = max_gaps <= max_gap_days\n",
    "    results['max_gap_days'] = max_gaps\n",
    "    results['gap_filter'] = gap_filter.astype(int)\n",
    "    print(f\"  Gap filter (≤{max_gap_days} consecutive): {gap_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 6: VOLATILITY SCREEN ===\n",
    "    # Remove extremely volatile stocks (potential data errors or penny stocks)\n",
    "    returns_vol = return_data.std(axis=0, skipna=True) * np.sqrt(252)\n",
    "    volatility_filter = (returns_vol <= volatility_threshold) | returns_vol.isna()\n",
    "    results['annualized_volatility'] = returns_vol\n",
    "    results['volatility_filter'] = volatility_filter.astype(int)\n",
    "    print(f\"  Volatility filter (≤{volatility_threshold*100:.0f}%): {volatility_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === FILTER 7: DATA COMPLETENESS ===\n",
    "    missing_returns = return_data.isnull().sum(axis=0)\n",
    "    max_missing = int(0.25 * lookback_days)  # Allow 25% missing data (was 15%)\n",
    "    completeness_filter = missing_returns <= max_missing\n",
    "    results['missing_returns'] = missing_returns\n",
    "    results['completeness_filter'] = completeness_filter.astype(int)\n",
    "    print(f\"  Completeness filter (≤{max_missing} missing): {completeness_filter.sum()}/{len(stocks)} stocks pass\")\n",
    "    \n",
    "    # === COMBINE ALL FILTERS ===\n",
    "    all_filters = [\n",
    "        'mcap_filter', 'price_filter', 'volume_filter', \n",
    "        'trading_filter', 'gap_filter', 'volatility_filter', 'completeness_filter'\n",
    "    ]\n",
    "    results['binary'] = results[all_filters].min(axis=1)\n",
    "    \n",
    "    final_count = results['binary'].sum()\n",
    "    print(f\"  ✅ FINAL: {final_count}/{len(stocks)} stocks pass ALL filters\")\n",
    "    \n",
    "    # Debug: Show what's failing\n",
    "    if final_count < 5:\n",
    "        print(f\"  📊 FILTER BREAKDOWN (showing restrictive filters):\")\n",
    "        for filter_name in all_filters:\n",
    "            count = results[filter_name].sum()\n",
    "            if count < len(stocks) * 0.8:  # Show filters that eliminate >20% of stocks\n",
    "                print(f\"    {filter_name}: {count}/{len(stocks)} ({count/len(stocks)*100:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def bibfn_selection_jkp_factor_quality(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter stocks based on JKP factor data availability and quality\n",
    "    \"\"\"\n",
    "    # Key factors for quality assessment\n",
    "    key_factors = kwargs.get('key_factors', [\n",
    "        'be_me', 'market_equity', 'ret_12_1', 'ivol_capm_252d', \n",
    "        'qmj', 'ni_me', 'at_me', 'beta_60m'\n",
    "    ])\n",
    "    min_factor_coverage = kwargs.get('min_factor_coverage', 0.6)  # 60% of factors must be available\n",
    "    \n",
    "    # Get current selection\n",
    "    current_stocks = bs.selection.selected if hasattr(bs.selection, 'selected') else []\n",
    "    if not current_stocks:\n",
    "        # If no previous selection, use all stocks with recent data\n",
    "        jkp_recent = bs.data.jkp_data[\n",
    "            bs.data.jkp_data.index.get_level_values('date') <= rebdate\n",
    "        ].groupby('id').last()\n",
    "        current_stocks = jkp_recent.index.tolist()\n",
    "    \n",
    "    results = pd.DataFrame(index=current_stocks)\n",
    "    \n",
    "    if not hasattr(bs.data, 'jkp_data') or bs.data.jkp_data is None:\n",
    "        print(\"  ⚠️ No JKP factor data available - skipping factor quality filter\")\n",
    "        results['binary'] = 1\n",
    "        return results\n",
    "    \n",
    "    # Get latest JKP data for each stock\n",
    "    jkp_recent = bs.data.jkp_data[\n",
    "        bs.data.jkp_data.index.get_level_values('date') <= rebdate\n",
    "    ].groupby('id').last()\n",
    "    \n",
    "    # Check availability of key factors\n",
    "    factor_availability = jkp_recent[key_factors].notna().sum(axis=1) / len(key_factors)\n",
    "    quality_filter = factor_availability >= min_factor_coverage\n",
    "    \n",
    "    results['factor_coverage'] = factor_availability.reindex(current_stocks, fill_value=0)\n",
    "    results['binary'] = quality_filter.reindex(current_stocks, fill_value=0).astype(int)\n",
    "    \n",
    "    passed = results['binary'].sum()\n",
    "    print(f\"  JKP factor quality: {passed}/{len(current_stocks)} stocks pass\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MOMENTUM AND TREND-BASED SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def bibfn_selection_momentum_screen(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select stocks with positive momentum characteristics\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    momentum_lookback = kwargs.get('momentum_lookback', [63, 126, 252])  # 3, 6, 12 months\n",
    "    min_positive_momentum = kwargs.get('min_positive_momentum', 2)  # At least 2 periods positive\n",
    "    exclude_extreme_momentum = kwargs.get('exclude_extreme_momentum', True)\n",
    "    momentum_threshold = kwargs.get('momentum_threshold', [-0.5, 2.0])  # -50% to +200%\n",
    "    \n",
    "    # Get current selection\n",
    "    current_stocks = bs.selection.selected if hasattr(bs.selection, 'selected') else []\n",
    "    if not current_stocks:\n",
    "        return_data = bs.data.get_return_series(end_date=rebdate, width=300)\n",
    "        current_stocks = return_data.columns.tolist()\n",
    "    \n",
    "    results = pd.DataFrame(index=current_stocks)\n",
    "    \n",
    "    # Get return data\n",
    "    return_data = bs.data.get_return_series(\n",
    "        ids=current_stocks, end_date=rebdate, width=300, fillna_value=0\n",
    "    )\n",
    "    \n",
    "    # Calculate momentum for different periods\n",
    "    cum_returns = (1 + return_data).cumprod()\n",
    "    momentum_signals = []\n",
    "    \n",
    "    for days in momentum_lookback:\n",
    "        momentum = cum_returns.iloc[-1] / cum_returns.iloc[-days-1] - 1\n",
    "        momentum_signals.append(momentum)\n",
    "        results[f'momentum_{days}d'] = momentum\n",
    "    \n",
    "    # Count positive momentum periods\n",
    "    momentum_df = pd.DataFrame(momentum_signals).T\n",
    "    positive_count = (momentum_df > 0).sum(axis=1)\n",
    "    results['positive_momentum_count'] = positive_count\n",
    "    \n",
    "    # Apply filters\n",
    "    momentum_filter = positive_count >= min_positive_momentum\n",
    "    \n",
    "    if exclude_extreme_momentum:\n",
    "        # Exclude stocks with extreme momentum (potential data errors)\n",
    "        extreme_filter = momentum_df.apply(\n",
    "            lambda x: ((x >= momentum_threshold[0]) & (x <= momentum_threshold[1])).all(), \n",
    "            axis=1\n",
    "        )\n",
    "        momentum_filter = momentum_filter & extreme_filter\n",
    "        results['extreme_filter'] = extreme_filter.astype(int)\n",
    "    \n",
    "    results['binary'] = momentum_filter.astype(int)\n",
    "    \n",
    "    passed = results['binary'].sum()\n",
    "    print(f\"  Momentum screen: {passed}/{len(current_stocks)} stocks pass\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. RISK-BASED SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def bibfn_selection_risk_screen(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out high-risk stocks based on multiple risk metrics\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    max_volatility = kwargs.get('max_volatility', 0.4)  # 40% annual\n",
    "    max_beta = kwargs.get('max_beta', 2.0)\n",
    "    min_observations = kwargs.get('min_observations', 150)  # Min days for risk calc\n",
    "    lookback_days = kwargs.get('lookback_days', 252)\n",
    "    \n",
    "    # Get current selection\n",
    "    current_stocks = bs.selection.selected if hasattr(bs.selection, 'selected') else []\n",
    "    if not current_stocks:\n",
    "        return_data = bs.data.get_return_series(end_date=rebdate, width=lookback_days)\n",
    "        current_stocks = return_data.columns.tolist()\n",
    "    \n",
    "    results = pd.DataFrame(index=current_stocks)\n",
    "    \n",
    "    # Get return data\n",
    "    stock_returns = bs.data.get_return_series(\n",
    "        ids=current_stocks, end_date=rebdate, width=lookback_days, fillna_value=None\n",
    "    )\n",
    "    \n",
    "    # Get benchmark returns (SPI)\n",
    "    benchmark_returns = bs.data.bm_series[\n",
    "        bs.data.bm_series.index <= rebdate\n",
    "    ].tail(lookback_days)\n",
    "    \n",
    "    # Align dates\n",
    "    common_dates = stock_returns.index.intersection(benchmark_returns.index)\n",
    "    stock_returns = stock_returns.loc[common_dates]\n",
    "    benchmark_returns = benchmark_returns.loc[common_dates]\n",
    "    \n",
    "    # Calculate risk metrics\n",
    "    for stock in current_stocks:\n",
    "        stock_ret = stock_returns[stock].dropna()\n",
    "        bench_ret = benchmark_returns.reindex(stock_ret.index).dropna()\n",
    "        \n",
    "        # Align series\n",
    "        common_idx = stock_ret.index.intersection(bench_ret.index)\n",
    "        if len(common_idx) < min_observations:\n",
    "            results.loc[stock, 'volatility'] = np.nan\n",
    "            results.loc[stock, 'beta'] = np.nan\n",
    "            results.loc[stock, 'observations'] = len(common_idx)\n",
    "            continue\n",
    "            \n",
    "        stock_aligned = stock_ret.loc[common_idx]\n",
    "        bench_aligned = bench_ret.loc[common_idx]\n",
    "        \n",
    "        # Calculate volatility\n",
    "        vol = stock_aligned.std() * np.sqrt(252)\n",
    "        results.loc[stock, 'volatility'] = vol\n",
    "        \n",
    "        # Calculate beta\n",
    "        covariance = np.cov(stock_aligned, bench_aligned)[0, 1]\n",
    "        benchmark_var = bench_aligned.var()\n",
    "        beta = covariance / benchmark_var if benchmark_var > 0 else np.nan\n",
    "        results.loc[stock, 'beta'] = beta\n",
    "        results.loc[stock, 'observations'] = len(common_idx)\n",
    "    \n",
    "    # Apply filters\n",
    "    vol_filter = (results['volatility'] <= max_volatility) | results['volatility'].isna()\n",
    "    beta_filter = (results['beta'] <= max_beta) | results['beta'].isna()\n",
    "    obs_filter = results['observations'] >= min_observations\n",
    "    \n",
    "    results['vol_filter'] = vol_filter.astype(int)\n",
    "    results['beta_filter'] = beta_filter.astype(int)\n",
    "    results['obs_filter'] = obs_filter.astype(int)\n",
    "    \n",
    "    # Combined filter\n",
    "    results['binary'] = (vol_filter & beta_filter & obs_filter).astype(int)\n",
    "    \n",
    "    passed = results['binary'].sum()\n",
    "    print(f\"  Risk screen: {passed}/{len(current_stocks)} stocks pass\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. SECTOR AND SIZE-BASED SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def bibfn_selection_size_tiers(bs, rebdate: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Advanced size-based selection with tier preferences\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    large_cap_min = kwargs.get('large_cap_min', 10_000_000_000)   # 10B CHF\n",
    "    mid_cap_min = kwargs.get('mid_cap_min', 1_000_000_000)        # 1B CHF\n",
    "    small_cap_min = kwargs.get('small_cap_min', 300_000_000)      # 300M CHF\n",
    "    \n",
    "    # Tier preferences (how many stocks from each tier)\n",
    "    max_large_cap = kwargs.get('max_large_cap', 20)\n",
    "    max_mid_cap = kwargs.get('max_mid_cap', 30)\n",
    "    max_small_cap = kwargs.get('max_small_cap', 15)\n",
    "    \n",
    "    # Get current selection\n",
    "    current_stocks = bs.selection.selected if hasattr(bs.selection, 'selected') else []\n",
    "    if not current_stocks:\n",
    "        mcap_data = bs.data.market_data['mktcap']\n",
    "        mcap_recent = mcap_data[\n",
    "            mcap_data.index.get_level_values('date') <= rebdate\n",
    "        ].groupby('id').last()\n",
    "        current_stocks = mcap_recent.index.tolist()\n",
    "    \n",
    "    # Get market cap data\n",
    "    mcap_data = bs.data.market_data['mktcap']\n",
    "    mcap_recent = mcap_data[\n",
    "        mcap_data.index.get_level_values('date') <= rebdate\n",
    "    ].groupby('id').last()\n",
    "    \n",
    "    results = pd.DataFrame(index=current_stocks)\n",
    "    results['market_cap'] = mcap_recent.reindex(current_stocks, fill_value=0)\n",
    "    \n",
    "    # Classify size tiers\n",
    "    results['size_tier'] = 'micro'\n",
    "    results.loc[results['market_cap'] >= small_cap_min, 'size_tier'] = 'small'\n",
    "    results.loc[results['market_cap'] >= mid_cap_min, 'size_tier'] = 'mid'\n",
    "    results.loc[results['market_cap'] >= large_cap_min, 'size_tier'] = 'large'\n",
    "    \n",
    "    # Apply tier limits\n",
    "    results['binary'] = 0\n",
    "    \n",
    "    # Select top stocks from each tier by market cap\n",
    "    for tier, max_count in [('large', max_large_cap), ('mid', max_mid_cap), ('small', max_small_cap)]:\n",
    "        tier_stocks = results[results['size_tier'] == tier].sort_values('market_cap', ascending=False)\n",
    "        selected_count = min(len(tier_stocks), max_count)\n",
    "        if selected_count > 0:\n",
    "            results.loc[tier_stocks.index[:selected_count], 'binary'] = 1\n",
    "    \n",
    "    # Summary\n",
    "    tier_summary = results.groupby('size_tier')['binary'].sum()\n",
    "    print(f\"  Size tier selection:\")\n",
    "    for tier, count in tier_summary.items():\n",
    "        print(f\"    {tier.capitalize()}: {count} stocks\")\n",
    "    \n",
    "    total_selected = results['binary'].sum()\n",
    "    print(f\"  Total selected: {total_selected}/{len(current_stocks)} stocks\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. COMBINED SELECTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def create_enhanced_selection_builders():\n",
    "    \"\"\"\n",
    "    Create the complete set of enhanced selection item builders with REASONABLE parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    selection_item_builders = {\n",
    "        # Core quality filters - RELAXED PARAMETERS\n",
    "        'comprehensive_quality': SelectionItemBuilder(\n",
    "            bibfn=bibfn_selection_comprehensive_quality,\n",
    "            min_volume=300_000,          # 300K CHF daily volume (relaxed from 1M)\n",
    "            min_price=1.0,               # Min 1 CHF per share (relaxed from 3)\n",
    "            max_price=2000.0,            # Max 2000 CHF per share (increased from 1000)\n",
    "            min_market_cap=50_000_000,   # 50M CHF market cap (relaxed from 200M)\n",
    "            min_trading_days=120,        # At least 120 trading days (relaxed from 180)\n",
    "            max_gap_days=30,             # Max 30 consecutive gaps (relaxed from 15)\n",
    "            volatility_threshold=1.2,    # Max 120% annual volatility (relaxed from 80%)\n",
    "        ),\n",
    "        \n",
    "        # JKP factor data quality\n",
    "        'jkp_factor_quality': SelectionItemBuilder(\n",
    "            bibfn=bibfn_selection_jkp_factor_quality,\n",
    "            key_factors=[\n",
    "                'be_me', 'market_equity', 'ret_12_1', 'ivol_capm_252d',\n",
    "                'qmj', 'ni_me', 'beta_60m', 'ret_6_1'  # Reduced list of core factors\n",
    "            ],\n",
    "            min_factor_coverage=0.4,  # 40% of key factors must be available (relaxed from 50%)\n",
    "        ),\n",
    "        \n",
    "        # Momentum screen - RELAXED\n",
    "        'momentum_screen': SelectionItemBuilder(\n",
    "            bibfn=bibfn_selection_momentum_screen,\n",
    "            momentum_lookback=[63, 126, 252],     # 3, 6, 12 months\n",
    "            min_positive_momentum=0,              # Allow any momentum (relaxed from 1)\n",
    "            exclude_extreme_momentum=True,\n",
    "            momentum_threshold=[-0.9, 5.0],       # Very wide range (relaxed)\n",
    "        ),\n",
    "        \n",
    "        # Risk management - RELAXED\n",
    "        'risk_screen': SelectionItemBuilder(\n",
    "            bibfn=bibfn_selection_risk_screen,\n",
    "            max_volatility=1.5,          # 150% annual volatility (relaxed from 60%)\n",
    "            max_beta=3.0,                # Beta limit (relaxed from 2.5)\n",
    "            min_observations=100,        # Min observations (relaxed from 120)\n",
    "        ),\n",
    "        \n",
    "        # Size-based selection - MORE GENEROUS\n",
    "        'size_tiers': SelectionItemBuilder(\n",
    "            bibfn=bibfn_selection_size_tiers,\n",
    "            max_large_cap=20,            # Max 20 large cap stocks\n",
    "            max_mid_cap=30,              # Max 30 mid cap stocks  \n",
    "            max_small_cap=15,            # Max 15 small cap stocks\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return selection_item_builders\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TESTING THE ENHANCED SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def test_enhanced_selection_fixed(data, test_date='2024-02-07'):\n",
    "    \"\"\"\n",
    "    Test the enhanced selection pipeline - FIXED VERSION\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING ENHANCED SELECTION PIPELINE - FIXED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Import functions directly (since they should be in the same environment)\n",
    "    #from assignment5_selection import create_enhanced_selection_builders\n",
    "    \n",
    "    # Create a proper test backtest service\n",
    "    class TestSelection:\n",
    "        def __init__(self):\n",
    "            self.selected = []\n",
    "            self.filtered = {}\n",
    "            \n",
    "        def add_filtered(self, name, value):\n",
    "            self.filtered[name] = value\n",
    "            # Update selected list based on binary column\n",
    "            if 'binary' in value.columns:\n",
    "                new_selected = value[value['binary'] == 1].index.tolist()\n",
    "                self.selected = new_selected\n",
    "                print(f\"    → Updated selection: {len(self.selected)} stocks selected\")\n",
    "    \n",
    "    class TestBacktestService:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "            self.selection = TestSelection()\n",
    "    \n",
    "    # Create test service\n",
    "    bs_test = TestBacktestService(data)\n",
    "    \n",
    "    # Get enhanced builders\n",
    "    builders = create_enhanced_selection_builders()\n",
    "    \n",
    "    print(f\"Testing selection pipeline for date: {test_date}\")\n",
    "    print(f\"Available selection filters: {list(builders.keys())}\")\n",
    "    \n",
    "    # Test each filter sequentially\n",
    "    results_summary = {}\n",
    "    total_stocks = data.get_return_series().shape[1]\n",
    "    \n",
    "    for filter_name, builder in builders.items():\n",
    "        print(f\"\\n--- Testing: {filter_name.upper()} ---\")\n",
    "        try:\n",
    "            # Run the filter\n",
    "            filter_result = builder.arguments['bibfn'](bs_test, test_date, **builder.arguments)\n",
    "            \n",
    "            # Update selection\n",
    "            bs_test.selection.add_filtered(filter_name, filter_result)\n",
    "            \n",
    "            # Store results\n",
    "            passed = filter_result['binary'].sum() if 'binary' in filter_result.columns else 0\n",
    "            total = len(filter_result)\n",
    "            results_summary[filter_name] = {'passed': passed, 'total': total}\n",
    "            \n",
    "            print(f\"✅ {filter_name}: {passed}/{total} stocks passed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {filter_name}: {str(e)}\")\n",
    "            results_summary[filter_name] = {'passed': 0, 'total': 0, 'error': str(e)}\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SELECTION PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for filter_name, result in results_summary.items():\n",
    "        if 'error' in result:\n",
    "            print(f\"{filter_name:25}: ERROR - {result['error']}\")\n",
    "        else:\n",
    "            pct = (result['passed'] / result['total'] * 100) if result['total'] > 0 else 0\n",
    "            print(f\"{filter_name:25}: {result['passed']:3d}/{result['total']:3d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    final_selection = len(bs_test.selection.selected) if hasattr(bs_test.selection, 'selected') else 0\n",
    "    print(f\"\\n🎯 FINAL INVESTABLE UNIVERSE: {final_selection} stocks\")\n",
    "    \n",
    "    # Show the filter progression\n",
    "    print(f\"\\n📊 FILTER PROGRESSION:\")\n",
    "    print(f\"Starting universe: {total_stocks} stocks\")\n",
    "    \n",
    "    current_count = total_stocks\n",
    "    for filter_name, filter_data in bs_test.selection.filtered.items():\n",
    "        if 'binary' in filter_data.columns:\n",
    "            passed = filter_data['binary'].sum()\n",
    "            print(f\"  After {filter_name:20}: {passed:3d} stocks remaining\")\n",
    "            current_count = passed\n",
    "    \n",
    "    return bs_test, results_summary\n",
    "\n",
    "# Test the enhanced selection pipeline with your data\n",
    "print(\"🚀 Testing Enhanced Selection Pipeline - FIXED VERSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run the test\n",
    "try:\n",
    "    test_service, results = test_enhanced_selection_fixed(data, test_date='2024-02-07')\n",
    "    \n",
    "    # Get the selection builders for actual use\n",
    "    enhanced_builders = create_enhanced_selection_builders()\n",
    "    \n",
    "    print(f\"\\n🎯 ANALYSIS OF RESULTS:\")\n",
    "    \n",
    "    # Show what makes the most impact\n",
    "    restrictive_filters = []\n",
    "    for filter_name, result in results.items():\n",
    "        if 'error' not in result and result['total'] > 0:\n",
    "            pass_rate = result['passed'] / result['total']\n",
    "            if pass_rate < 0.5:  # Less than 50% pass rate\n",
    "                restrictive_filters.append((filter_name, pass_rate, result['passed']))\n",
    "    \n",
    "    if restrictive_filters:\n",
    "        print(f\"\\n🔍 MOST RESTRICTIVE FILTERS:\")\n",
    "        restrictive_filters.sort(key=lambda x: x[1])  # Sort by pass rate\n",
    "        for name, rate, count in restrictive_filters:\n",
    "            print(f\"  {name:25}: {rate*100:5.1f}% pass rate ({count} stocks)\")\n",
    "    \n",
    "    print(f\"\\n📈 RECOMMENDED INVESTABLE UNIVERSE:\")\n",
    "    final_count = len(test_service.selection.selected) if hasattr(test_service.selection, 'selected') else 0\n",
    "    print(f\"  Final selection: {final_count} stocks\")\n",
    "    print(f\"  Universe reduction: {313} → {final_count} ({final_count/313*100:.1f}% of original)\")\n",
    "    \n",
    "    if final_count > 20:\n",
    "        print(f\"  ✅ Good size for portfolio optimization ({final_count} stocks)\")\n",
    "    elif final_count > 10:\n",
    "        print(f\"  ⚠️  Small but workable universe ({final_count} stocks)\")\n",
    "    else:\n",
    "        print(f\"  ❌ Universe may be too small ({final_count} stocks)\")\n",
    "    \n",
    "    print(f\"\\n💡 READY FOR NEXT STEP:\")\n",
    "    print(f\"✓ Selection filters working correctly\")\n",
    "    print(f\"✓ {len(enhanced_builders)} selection builders ready\")\n",
    "    print(f\"✓ Investable universe defined: {final_count} stocks\")\n",
    "    print(f\"→ Next: Implement optimization data builders and ML features\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed: {e}\")\n",
    "    print(\"Please check that the selection functions are properly loaded\")\n",
    "zzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LSTM + Neural Network Return Prediction System\n",
      "==================================================\n",
      "Key Components:\n",
      "✓ StockReturnDataset - Time series data preparation\n",
      "✓ LSTMReturnPredictor - LSTM + NN architecture\n",
      "✓ LSTMTrainer - Training framework with early stopping\n",
      "✓ bibfn_lstm_return_prediction - Integration with backtesting\n",
      "✓ Alternative Keras implementation available\n",
      "\n",
      "Ready for integration with BacktestService!\n"
     ]
    }
   ],
   "source": [
    "# Assignment 5 - LSTM + Neural Network for Return Prediction\n",
    "# Deep Learning approach for Swiss equity return forecasting\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA PREPARATION FOR LSTM\n",
    "# =============================================================================\n",
    "\n",
    "class StockReturnDataset:\n",
    "    \"\"\"\n",
    "    Prepare time series data for LSTM + NN training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 return_data: pd.DataFrame,\n",
    "                 jkp_data: pd.DataFrame = None,\n",
    "                 technical_features: pd.DataFrame = None,\n",
    "                 sequence_length: int = 30,\n",
    "                 prediction_horizon: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize dataset for LSTM training\n",
    "        \n",
    "        Args:\n",
    "            return_data: Stock returns (dates x stocks)\n",
    "            jkp_data: JKP factor data (dates x stocks x factors) \n",
    "            technical_features: Technical indicators (dates x features)\n",
    "            sequence_length: Length of input sequences for LSTM\n",
    "            prediction_horizon: Days ahead to predict\n",
    "        \"\"\"\n",
    "        self.return_data = return_data\n",
    "        self.jkp_data = jkp_data\n",
    "        self.technical_features = technical_features\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        \n",
    "        self.scaler_features = RobustScaler()\n",
    "        self.scaler_targets = RobustScaler()\n",
    "        \n",
    "    def prepare_features_and_targets(self) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Create feature sequences and target returns for all stocks\n",
    "        \"\"\"\n",
    "        print(\"📊 Preparing LSTM dataset...\")\n",
    "        \n",
    "        dates = self.return_data.index\n",
    "        stocks = self.return_data.columns.tolist()\n",
    "        \n",
    "        # Create combined feature matrix for each date\n",
    "        feature_matrices = []\n",
    "        \n",
    "        for date in dates:\n",
    "            date_features = {}\n",
    "            \n",
    "            # 1. Historical returns (sequence_length x n_stocks)\n",
    "            date_idx = dates.get_loc(date)\n",
    "            start_idx = max(0, date_idx - self.sequence_length + 1)\n",
    "            historical_returns = self.return_data.iloc[start_idx:date_idx+1]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(historical_returns) < self.sequence_length:\n",
    "                padding = pd.DataFrame(\n",
    "                    0, \n",
    "                    index=pd.date_range(end=date, periods=self.sequence_length-len(historical_returns)),\n",
    "                    columns=stocks\n",
    "                )\n",
    "                historical_returns = pd.concat([padding, historical_returns])\n",
    "            \n",
    "            date_features['returns'] = historical_returns.values  # (seq_len, n_stocks)\n",
    "            \n",
    "            # 2. JKP factors (if available)\n",
    "            if self.jkp_data is not None:\n",
    "                # Get JKP data for this date (or most recent)\n",
    "                jkp_for_date = self.jkp_data[\n",
    "                    self.jkp_data.index.get_level_values('date') <= date\n",
    "                ].groupby('id').last()\n",
    "                \n",
    "                # Align with stocks and fill missing\n",
    "                jkp_aligned = jkp_for_date.reindex(stocks).fillna(0)\n",
    "                date_features['jkp'] = jkp_aligned.values  # (n_stocks, n_factors)\n",
    "            \n",
    "            # 3. Technical features (if available)\n",
    "            if self.technical_features is not None and date in self.technical_features.index:\n",
    "                tech_features = self.technical_features.loc[date].values\n",
    "                date_features['technical'] = tech_features  # (n_tech_features,)\n",
    "            \n",
    "            feature_matrices.append(date_features)\n",
    "        \n",
    "        # Create sequences and targets\n",
    "        X_sequences = []\n",
    "        y_targets = []\n",
    "        valid_dates = []\n",
    "        \n",
    "        for i in range(self.sequence_length, len(dates) - self.prediction_horizon):\n",
    "            # Features: sequence of past data\n",
    "            sequence_data = feature_matrices[i-self.sequence_length:i]\n",
    "            \n",
    "            # Combine features for this sequence\n",
    "            combined_features = self._combine_sequence_features(sequence_data, stocks)\n",
    "            \n",
    "            # Target: future returns\n",
    "            future_start = i + 1\n",
    "            future_end = i + self.prediction_horizon + 1\n",
    "            future_returns = self.return_data.iloc[future_start:future_end].mean(axis=0)\n",
    "            \n",
    "            if not future_returns.isna().all():\n",
    "                X_sequences.append(combined_features)\n",
    "                y_targets.append(future_returns.fillna(0).values)\n",
    "                valid_dates.append(dates[i])\n",
    "        \n",
    "        X = np.array(X_sequences)  # (n_samples, seq_len, n_features)\n",
    "        y = np.array(y_targets)    # (n_samples, n_stocks)\n",
    "        \n",
    "        print(f\"  Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "        print(f\"  Features per timestep: {X.shape[2]}\")\n",
    "        print(f\"  Sequence length: {X.shape[1]}\")\n",
    "        \n",
    "        return X, y, stocks, valid_dates\n",
    "    \n",
    "    def _combine_sequence_features(self, sequence_data: List[Dict], stocks: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Combine different feature types for a single sequence\n",
    "        \"\"\"\n",
    "        n_timesteps = len(sequence_data)\n",
    "        combined = []\n",
    "        \n",
    "        for t in range(n_timesteps):\n",
    "            timestep_features = []\n",
    "            \n",
    "            # Returns for all stocks\n",
    "            if 'returns' in sequence_data[t]:\n",
    "                returns_t = sequence_data[t]['returns'][-1]  # Latest returns\n",
    "                timestep_features.extend(returns_t)\n",
    "            \n",
    "            # JKP factors (use same for all timesteps in sequence - they're monthly)\n",
    "            if 'jkp' in sequence_data[t]:\n",
    "                jkp_t = sequence_data[t]['jkp'].flatten()\n",
    "                timestep_features.extend(jkp_t)\n",
    "            \n",
    "            # Technical features (market-wide)\n",
    "            if 'technical' in sequence_data[t]:\n",
    "                tech_t = sequence_data[t]['technical']\n",
    "                timestep_features.extend(tech_t)\n",
    "            \n",
    "            combined.append(timestep_features)\n",
    "        \n",
    "        return np.array(combined)  # (seq_len, n_features)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LSTM + NEURAL NETWORK ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMReturnPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Encoder + Dense Neural Network for return prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 hidden_size: int = 64,\n",
    "                 num_layers: int = 2,\n",
    "                 num_stocks: int = 48,\n",
    "                 dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize LSTM + NN architecture\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of features per timestep\n",
    "            hidden_size: LSTM hidden units\n",
    "            num_layers: Number of LSTM layers\n",
    "            num_stocks: Number of stocks to predict\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(LSTMReturnPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_stocks = num_stocks\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Dense layers for prediction\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        \n",
    "        self.dense2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, num_stocks)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: (batch_size, num_stocks)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.dropout(last_output)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        # Output predictions\n",
    "        predictions = self.output(x)\n",
    "        predictions = self.tanh(predictions) * 0.1  # Scale to reasonable return range\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAINING FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMTrainer:\n",
    "    \"\"\"\n",
    "    Training framework for LSTM return prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: LSTMReturnPredictor,\n",
    "                 learning_rate: float = 0.001,\n",
    "                 weight_decay: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Initialize trainer\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Track training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def train_epoch(self, train_loader: DataLoader, device: torch.device) -> float:\n",
    "        \"\"\"\n",
    "        Train for one epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.criterion(predictions, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def validate(self, val_loader: DataLoader, device: torch.device) -> float:\n",
    "        \"\"\"\n",
    "        Validate the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                predictions = self.model(data)\n",
    "                loss = self.criterion(predictions, targets)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(val_loader)\n",
    "    \n",
    "    def train(self, \n",
    "              train_loader: DataLoader, \n",
    "              val_loader: DataLoader,\n",
    "              epochs: int = 100,\n",
    "              early_stopping_patience: int = 15,\n",
    "              device: torch.device = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Full training loop with early stopping\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model.to(device)\n",
    "        \n",
    "        print(f\"🚀 Training LSTM model on {device}\")\n",
    "        print(f\"  Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_loader, device)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate(val_loader, device)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Track losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0 or epoch < 10:\n",
    "                print(f\"  Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"  Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "        \n",
    "        print(f\"✅ Training completed. Best validation loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'epochs_trained': len(self.train_losses)\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INTEGRATION WITH BACKTESTING FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "def bibfn_lstm_return_prediction(bs, rebdate: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    LSTM-based return prediction for optimization\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    sequence_length = kwargs.get('sequence_length', 30)  # 30 trading days\n",
    "    prediction_horizon = kwargs.get('prediction_horizon', 5)  # 5 days ahead\n",
    "    hidden_size = kwargs.get('hidden_size', 64)\n",
    "    num_layers = kwargs.get('num_layers', 2)\n",
    "    train_window = kwargs.get('train_window', 756)  # 3 years\n",
    "    retrain_frequency = kwargs.get('retrain_frequency', 63)  # Quarterly\n",
    "    \n",
    "    selected_stocks = bs.selection.selected\n",
    "    if not selected_stocks:\n",
    "        print(\"  ⚠️  No stocks selected - skipping LSTM prediction\")\n",
    "        return\n",
    "    \n",
    "    if not TORCH_AVAILABLE:\n",
    "        print(\"  ⚠️  PyTorch not available - using fallback prediction\")\n",
    "        # Fallback to simple momentum\n",
    "        return_data = bs.data.get_return_series(\n",
    "            ids=selected_stocks, end_date=rebdate, width=126\n",
    "        )\n",
    "        momentum = (1 + return_data.tail(63)).prod() - 1\n",
    "        predictions = momentum * 0.05  # Scale down\n",
    "        bs.optimization_data['return_predictions'] = predictions\n",
    "        return\n",
    "    \n",
    "    print(f\"🧠 LSTM Return Prediction for {len(selected_stocks)} stocks\")\n",
    "    \n",
    "    # Check if we need to retrain model\n",
    "    model_key = f'lstm_model_{rebdate}'\n",
    "    should_retrain = True  # For now, always retrain (could optimize this)\n",
    "    \n",
    "    if should_retrain:\n",
    "        print(\"  Training new LSTM model...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        return_data = bs.data.get_return_series(\n",
    "            ids=selected_stocks, end_date=rebdate, width=train_window\n",
    "        )\n",
    "        \n",
    "        # Get JKP data if available\n",
    "        jkp_data = None\n",
    "        if hasattr(bs.data, 'jkp_data') and bs.data.jkp_data is not None:\n",
    "            jkp_data = bs.data.jkp_data\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = StockReturnDataset(\n",
    "            return_data=return_data,\n",
    "            jkp_data=jkp_data,\n",
    "            sequence_length=sequence_length,\n",
    "            prediction_horizon=prediction_horizon\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Prepare features and targets\n",
    "            X, y, stock_names, valid_dates = dataset.prepare_features_and_targets()\n",
    "            \n",
    "            if len(X) < 100:  # Need sufficient training data\n",
    "                print(\"  ⚠️  Insufficient data for LSTM training - using momentum fallback\")\n",
    "                momentum = (1 + return_data.tail(63)).prod() - 1\n",
    "                predictions = momentum * 0.03\n",
    "                bs.optimization_data['return_predictions'] = predictions\n",
    "                return\n",
    "            \n",
    "            # Train-validation split (time series)\n",
    "            split_idx = int(0.8 * len(X))\n",
    "            X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_train = torch.FloatTensor(X_train)\n",
    "            y_train = torch.FloatTensor(y_train)\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "            y_val = torch.FloatTensor(y_val)\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            val_dataset = TensorDataset(X_val, y_val)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # Don't shuffle time series\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "            \n",
    "            # Create model\n",
    "            input_size = X.shape[2]  # Features per timestep\n",
    "            num_stocks = len(selected_stocks)\n",
    "            \n",
    "            model = LSTMReturnPredictor(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                num_stocks=num_stocks,\n",
    "                dropout=0.2\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            trainer = LSTMTrainer(model, learning_rate=0.001)\n",
    "            training_history = trainer.train(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader, \n",
    "                epochs=50,  # Reduced for speed\n",
    "                early_stopping_patience=10\n",
    "            )\n",
    "            \n",
    "            # Make predictions for current date\n",
    "            # Use the most recent sequence for prediction\n",
    "            latest_sequence = X[-1:]  # Last sequence\n",
    "            latest_tensor = torch.FloatTensor(latest_sequence)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions_tensor = model(latest_tensor)\n",
    "                predictions_array = predictions_tensor.numpy().flatten()\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            predictions = pd.Series(predictions_array, index=selected_stocks)\n",
    "            \n",
    "            # Store results\n",
    "            bs.optimization_data['return_predictions'] = predictions\n",
    "            bs.optimization_data['lstm_model'] = model\n",
    "            bs.optimization_data['lstm_training_history'] = training_history\n",
    "            \n",
    "            print(f\"  ✅ LSTM training completed:\")\n",
    "            print(f\"    Final validation loss: {training_history['best_val_loss']:.6f}\")\n",
    "            print(f\"    Prediction range: {predictions.min():.4f} to {predictions.max():.4f}\")\n",
    "            print(f\"    Mean prediction: {predictions.mean():.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ LSTM training failed: {e}\")\n",
    "            print(\"  Falling back to momentum strategy...\")\n",
    "            \n",
    "            # Fallback prediction\n",
    "            momentum = (1 + return_data.tail(63)).prod() - 1\n",
    "            predictions = momentum * 0.02\n",
    "            bs.optimization_data['return_predictions'] = predictions.fillna(0)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. ALTERNATIVE: TensorFlow/Keras Implementation\n",
    "# =============================================================================\n",
    "\n",
    "def create_keras_lstm_model(input_shape: Tuple[int, int], num_stocks: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Create LSTM model using Keras (alternative to PyTorch)\n",
    "    \"\"\"\n",
    "    if not TF_AVAILABLE:\n",
    "        raise ImportError(\"TensorFlow not available\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # LSTM layers\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_stocks, activation='tanh')  # tanh to bound outputs\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def bibfn_keras_lstm_prediction(bs, rebdate: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Keras/TensorFlow LSTM implementation (alternative)\n",
    "    \"\"\"\n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"  ⚠️  TensorFlow not available - skipping Keras LSTM\")\n",
    "        return\n",
    "    \n",
    "    # Similar implementation to PyTorch version but using Keras\n",
    "    # This would follow the same pattern but with TensorFlow/Keras syntax\n",
    "    print(\"  🧠 Keras LSTM implementation (placeholder)\")\n",
    "    \n",
    "    # For now, redirect to PyTorch version or implement later\n",
    "    return bibfn_lstm_return_prediction(bs, rebdate, **kwargs)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TESTING AND INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def test_lstm_prediction(data, selected_stocks: List[str], test_date: str = '2024-02-07'):\n",
    "    \"\"\"\n",
    "    Test the LSTM prediction system\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING LSTM RETURN PREDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not TORCH_AVAILABLE:\n",
    "        print(\"❌ PyTorch not available - install with: pip install torch\")\n",
    "        return None\n",
    "    \n",
    "    # Create test service\n",
    "    class TestLSTMService:\n",
    "        def __init__(self, data, selected_stocks):\n",
    "            self.data = data\n",
    "            self.selection = type('Selection', (), {'selected': selected_stocks})()\n",
    "            self.optimization_data = {}\n",
    "    \n",
    "    bs_test = TestLSTMService(data, selected_stocks)\n",
    "    \n",
    "    print(f\"Testing LSTM with {len(selected_stocks)} stocks\")\n",
    "    \n",
    "    try:\n",
    "        # Test LSTM prediction\n",
    "        bibfn_lstm_return_prediction(\n",
    "            bs_test, \n",
    "            test_date,\n",
    "            sequence_length=20,  # Shorter for testing\n",
    "            hidden_size=32,      # Smaller for speed\n",
    "            num_layers=1,        # Single layer for testing\n",
    "            train_window=500     # Shorter training window\n",
    "        )\n",
    "        \n",
    "        if 'return_predictions' in bs_test.optimization_data:\n",
    "            predictions = bs_test.optimization_data['return_predictions']\n",
    "            print(f\"  ✅ LSTM predictions generated:\")\n",
    "            print(f\"    Shape: {predictions.shape}\")\n",
    "            print(f\"    Range: {predictions.min():.4f} to {predictions.max():.4f}\")\n",
    "            print(f\"    Mean: {predictions.mean():.4f}\")\n",
    "            print(f\"    Std: {predictions.std():.4f}\")\n",
    "            \n",
    "            # Show top and bottom predictions\n",
    "            print(f\"\\n  📈 Top 5 predicted returns:\")\n",
    "            print(predictions.nlargest(5))\n",
    "            print(f\"\\n  📉 Bottom 5 predicted returns:\")\n",
    "            print(predictions.nsmallest(5))\n",
    "            \n",
    "            return predictions\n",
    "        else:\n",
    "            print(\"  ❌ No predictions generated\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ LSTM test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Usage example for integration\n",
    "def create_lstm_optimization_builders():\n",
    "    \"\"\"\n",
    "    Create optimization item builders with LSTM prediction\n",
    "    \"\"\"\n",
    "    from backtesting.backtest_item_builder_classes import OptimizationItemBuilder\n",
    "    from backtesting.backtest_item_builder_functions import (\n",
    "        bibfn_return_series, bibfn_budget_constraint, bibfn_box_constraints\n",
    "    )\n",
    "    \n",
    "    optimization_item_builders = {\n",
    "        # Core data\n",
    "        'return_series': OptimizationItemBuilder(\n",
    "            bibfn=bibfn_return_series,\n",
    "            width=252*3,\n",
    "            fill_value=0,\n",
    "        ),\n",
    "        \n",
    "        # LSTM Predictions - THE MAIN ML COMPONENT\n",
    "        'lstm_predictions': OptimizationItemBuilder(\n",
    "            bibfn=bibfn_lstm_return_prediction,\n",
    "            sequence_length=30,        # 30-day sequences\n",
    "            prediction_horizon=5,      # 5-day ahead prediction\n",
    "            hidden_size=64,           # LSTM hidden units\n",
    "            num_layers=2,             # LSTM layers\n",
    "            train_window=756,         # 3 years training\n",
    "            retrain_frequency=63,     # Retrain quarterly\n",
    "        ),\n",
    "        \n",
    "        # Constraints\n",
    "        'budget_constraint': OptimizationItemBuilder(\n",
    "            bibfn=bibfn_budget_constraint,\n",
    "            budget=1.0,\n",
    "        ),\n",
    "        \n",
    "        'box_constraints': OptimizationItemBuilder(\n",
    "            bibfn=bibfn_box_constraints,\n",
    "            lower=0.0,\n",
    "            upper=0.15,  # Max 15% per stock\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return optimization_item_builders\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 LSTM + Neural Network Return Prediction System\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Key Components:\")\n",
    "    print(\"✓ StockReturnDataset - Time series data preparation\")\n",
    "    print(\"✓ LSTMReturnPredictor - LSTM + NN architecture\") \n",
    "    print(\"✓ LSTMTrainer - Training framework with early stopping\")\n",
    "    print(\"✓ bibfn_lstm_return_prediction - Integration with backtesting\")\n",
    "    print(\"✓ Alternative Keras implementation available\")\n",
    "    print(\"\\nReady for integration with BacktestService!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpmwp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
